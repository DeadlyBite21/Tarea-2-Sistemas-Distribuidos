services:
  kafka:
    image: redpandadata/redpanda:v24.1.3
    command: [
      "redpanda", "start",
      "--overprovisioned", "--smp", "1",
      "--reserve-memory", "0M",
      "--node-id", "0",
      "--kafka-addr", "PLAINTEXT://0.0.0.0:9092",
      "--advertise-kafka-addr", "PLAINTEXT://kafka:9092"
    ]
    ports: ["9092:9092"]

  flink-jobmanager:
    # --- Usa 'build' para crear la imagen personalizada ---
    build: 
      context: ./flink_job # Directorio con el Dockerfile de Flink
    user: flink # Vuelve a usar el usuario 'flink', Python ya está instalado
    volumes: 
      - ./flink_job:/opt/flink/job # Monta el código para desarrollo
    # --- Comando simplificado ---
    command: |
      bash -c "
      # Solo inicia jobmanager y envía el job
      /docker-entrypoint.sh jobmanager &
      JOBMANAGER_PID=$$!
      echo 'Esperando 10s para que el cluster inicie...';
      sleep 10; 
      echo 'Enviando trabajo de Flink (app.py)...';
      flink run -py /opt/flink/job/app.py;
      wait $$JOBMANAGER_PID
      "
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - BOOTSTRAP_SERVERS=kafka:9092
      - SCORE_THRESHOLD=0.62
      - TOPIC_INPUT=questions.answers
      - TOPIC_OUTPUT_VALIDATED=questions.validated
      - TOPIC_OUTPUT_REGENERATE=questions.llm
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: ${JOB_MANAGER_RPC_ADDRESS}
    ports: ["8081:8081"]
    depends_on: [kafka]

  flink-taskmanager:
    # --- Usa 'build' para crear la imagen personalizada ---
    build:
      context: ./flink_job # Usa el mismo Dockerfile
    user: flink # Vuelve a usar el usuario 'flink', Python ya está instalado
    volumes: 
      - ./flink_job:/opt/flink/job # Monta el código
    # --- Comando muy simplificado ---
    command: taskmanager # Solo inicia el taskmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: ${JOB_MANAGER_RPC_ADDRESS}
    depends_on: [flink-jobmanager]

  generator:
    build: ./generator
    environment:
      - STORAGE_SERVICE_URL=http://bdd:8000
    depends_on:
      - bdd
    restart: on-failure
      
  llm:
    build: ./llm
    ports:
      - "8003:8003"
    environment:
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - BOOTSTRAP_SERVERS=kafka:9092
      - TOPIC_REQUESTS=questions.llm 
    depends_on:
      - kafka

  retry-overload:
    build: ./retry
    command: python retry_overload.py
    environment:
    - BOOTSTRAP_SERVERS=kafka:9092
    depends_on: [kafka]

  retry-quota:
    build: ./retry
    command: python retry_quota.py
    environment:
    - BOOTSTRAP_SERVERS=kafka:9092
    depends_on: [kafka]

  bdd:
    build: ./bdd
    environment:
    - BOOTSTRAP_SERVERS=kafka:9092
    - DB_URL=sqlite:///data/data.db 
    volumes:
      - storage_data:/app/data
    ports:
      - "8001:8000" 
    depends_on: [kafka]

  kafdrop:
    image: obsidiandynamics/kafdrop:latest
    restart: "no"
    ports:
      - "9000:9000"
    environment:
      KAFKA_BROKERCONNECT: "kafka:9092"
      JVM_OPTS: "-Xms16M -Xmx48M -Xss180K -XX:-TieredCompilation -XX:+UseStringDeduplication -noverify"
    depends_on:
      - kafka

volumes:
  storage_data: {}